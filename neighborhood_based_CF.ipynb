{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Set up connection to Spark **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /user/wfeng is exceeded: quota = 5368709120 B = 5 GB but diskspace consumed = 5565257028 B = 5.18 GB\n\tat org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyStoragespaceQuota(DirectoryWithQuotaFeature.java:211)\n\tat org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:1073)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:902)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:861)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:567)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3803)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.storeAllocatedBlock(FSNamesystem.java:3387)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3268)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:850)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:503)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1577)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1369)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:558)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.DSQuotaExceededException): The DiskSpace quota of /user/wfeng is exceeded: quota = 5368709120 B = 5 GB but diskspace consumed = 5565257028 B = 5.18 GB\n\tat org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyStoragespaceQuota(DirectoryWithQuotaFeature.java:211)\n\tat org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:1073)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:902)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:861)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:567)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3803)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.storeAllocatedBlock(FSNamesystem.java:3387)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3268)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:850)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:503)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1498)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat com.sun.proxy.$Proxy12.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:459)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)\n\tat com.sun.proxy.$Proxy13.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1574)\n\t... 2 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d46d800cff6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m       .set(\"spark.executor.memory\", \"20g\"))\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHiveContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mhiveContext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mHiveContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark2-client/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m--> 118\u001b[1;33m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark2-client/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark2-client/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \"\"\"\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1399\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1400\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1401\u001b[1;33m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[0;32m   1402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1403\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /user/wfeng is exceeded: quota = 5368709120 B = 5 GB but diskspace consumed = 5565257028 B = 5.18 GB\n\tat org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyStoragespaceQuota(DirectoryWithQuotaFeature.java:211)\n\tat org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:1073)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:902)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:861)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:567)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3803)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.storeAllocatedBlock(FSNamesystem.java:3387)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3268)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:850)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:503)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1577)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1369)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:558)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.DSQuotaExceededException): The DiskSpace quota of /user/wfeng is exceeded: quota = 5368709120 B = 5 GB but diskspace consumed = 5565257028 B = 5.18 GB\n\tat org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyStoragespaceQuota(DirectoryWithQuotaFeature.java:211)\n\tat org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:1073)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:902)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:861)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:567)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3803)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.storeAllocatedBlock(FSNamesystem.java:3387)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3268)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:850)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:503)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1498)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat com.sun.proxy.$Proxy12.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:459)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)\n\tat com.sun.proxy.$Proxy13.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1574)\n\t... 2 more\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Basic  system Set up\n",
    "'''\n",
    "import time\n",
    "from time import gmtime, strftime \n",
    "timetmp = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n",
    "appname = 'Pyspark_' + timetmp\n",
    "\n",
    "import os\n",
    "\n",
    "## update the pyspark config\n",
    "os.environ[\"LD_LIBRARY_PATH\"]=\"/opt/rh/python27/root/usr/lib64\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/dsap/devl/private/common/python2.7/bin/python\"\n",
    "os.environ[\"SPARK_HOME\"]=\"/usr/hdp/current/spark2-client\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/dsap/devl/private/common/python2.7/bin/jupyter\"\n",
    "os.environ[\"PYTHONPATH\"]=\"/usr/hdp/current/spark2-client/python:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip\"\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf=(SparkConf().setMaster(\"yarn-client\")\n",
    "      .set(\"spark.yarn.queue\", \"BIA-support\")\n",
    "      .set(\"spark.num.executors\",\"20\")\n",
    "      .set(\"spark.driver.cores\",\"8\")\n",
    "      .set(\"spark.executor.memory\", \"20g\"))\n",
    "\n",
    "sc=SparkContext(conf=conf)\n",
    "from pyspark.sql import HiveContext\n",
    "hiveContext=HiveContext(sc)\n",
    "\n",
    "print \"done\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Use RMSE and MAE as evaluation metric and only focus on the 5 key merchants **\n",
    "- Walmart\n",
    "- Sam's Club\n",
    "- Amazon\n",
    "- Starbucks\n",
    "- CVS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Use raw (capped) transaction count data and select merchant neighbors based on Cosine similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 3 ms, total: 3 ms\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "query = 'select acct_key, mrch_brnd_nm, mrch_brnd_id, tran_cnt_cap \\\n",
    "         from s_card_marketinganalytics.wfeng_acct_tran_cap'\n",
    "%time df = hiveContext.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, validation, test = df.randomSplit([0.8,0.1,0.1], seed=1) # make sure we get the same split every time we run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hiveContext.registerDataFrameAsTable(train,'train_table')\n",
    "hiveContext.registerDataFrameAsTable(validation,'validation_table')\n",
    "hiveContext.registerDataFrameAsTable(test,'test_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save split tables to Hive\n",
    "%time hiveContext.sql('drop table if exists s_card_marketinganalytics.wfeng_train_table')\n",
    "%time hiveContext.sql('create table s_card_marketinganalytics.wfeng_train_table as select * from train_table')\n",
    "\n",
    "%time hiveContext.sql('drop table if exists s_card_marketinganalytics.wfeng_validation_table')\n",
    "%time hiveContext.sql('create table s_card_marketinganalytics.wfeng_validation_table as select * from validation_table')\n",
    "\n",
    "%time hiveContext.sql('drop table if exists s_card_marketinganalytics.wfeng_test_table')\n",
    "%time hiveContext.sql('create table s_card_marketinganalytics.wfeng_test_table as select * from test_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 µs, sys: 1 ms, total: 2 ms\n",
      "Wall time: 1.02 s\n",
      "CPU times: user 79 ms, sys: 30 ms, total: 109 ms\n",
      "Wall time: 11min 46s\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 10.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[mrch_brnd_nm: string, mrch_brnd_nm2: string, sim: double, num_acct: bigint]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create raw Cosine similarity between each 5 key merchants and all other merchants\n",
    "query = 'drop table if exists s_card_marketinganalytics.wfeng_5key_mrch_sim'\n",
    "%time hiveContext.sql(query)\n",
    "\n",
    "query = \"\"\"\n",
    "        CREATE TABLE s_card_marketinganalytics.wfeng_5key_mrch_sim AS\n",
    "        SELECT a.mrch_brnd_nm, b.mrch_brnd_nm AS mrch_brnd_nm2,\n",
    "        SUM(a.tran_cnt_cap * b.tran_cnt_cap)/(SQRT(SUM(a.tran_cnt_cap * a.tran_cnt_cap)) * SQRT(SUM(b.tran_cnt_cap * b.tran_cnt_cap))) AS sim\n",
    "        , COUNT(*) AS num_acct\n",
    "        FROM s_card_marketinganalytics.wfeng_train_table a\n",
    "        inner JOIN s_card_marketinganalytics.wfeng_train_table b\n",
    "        ON a.acct_key = b.acct_key\n",
    "        WHERE a.mrch_brnd_nm in ('WAL-MART', 'AMAZON.COM', 'STARBUCKS', 'CVS') or a.mrch_brnd_nm like 'SAM%S CLUB'\n",
    "        GROUP BY a.mrch_brnd_nm, b.mrch_brnd_nm\n",
    "        \"\"\"\n",
    "%time hiveContext.sql(query)\n",
    "\n",
    "query = 'select * from s_card_marketinganalytics.wfeng_5key_mrch_sim'\n",
    "%time hiveContext.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 10; k: 50\n",
      "compute Cosine similarity based on discount factor beta\n",
      "167 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "136 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "565 seconds\n",
      "calculate RMSE and MAE\n",
      "88 seconds\n",
      "beta: 10; k: 100\n",
      "compute Cosine similarity based on discount factor beta\n",
      "148 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "133 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "444 seconds\n",
      "calculate RMSE and MAE\n",
      "85 seconds\n",
      "beta: 10; k: 500\n",
      "compute Cosine similarity based on discount factor beta\n",
      "129 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "146 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "451 seconds\n",
      "calculate RMSE and MAE\n",
      "105 seconds\n",
      "beta: 50; k: 50\n",
      "compute Cosine similarity based on discount factor beta\n",
      "139 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "193 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "602 seconds\n",
      "calculate RMSE and MAE\n",
      "81 seconds\n",
      "beta: 50; k: 100\n",
      "compute Cosine similarity based on discount factor beta\n",
      "129 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "155 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "462 seconds\n",
      "calculate RMSE and MAE\n",
      "85 seconds\n",
      "beta: 50; k: 500\n",
      "compute Cosine similarity based on discount factor beta\n",
      "121 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "153 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "553 seconds\n",
      "calculate RMSE and MAE\n",
      "87 seconds\n",
      "beta: 100; k: 50\n",
      "compute Cosine similarity based on discount factor beta\n",
      "116 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "151 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "464 seconds\n",
      "calculate RMSE and MAE\n",
      "87 seconds\n",
      "beta: 100; k: 100\n",
      "compute Cosine similarity based on discount factor beta\n",
      "129 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "139 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "471 seconds\n",
      "calculate RMSE and MAE\n",
      "87 seconds\n",
      "beta: 100; k: 500\n",
      "compute Cosine similarity based on discount factor beta\n",
      "162 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "164 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "547 seconds\n",
      "calculate RMSE and MAE\n",
      "85 seconds\n",
      "optimal rmse: 6.28; best beta: 10; best k: 100\n",
      "optimal mae: 3.75; best beta: 10; best k: 100\n"
     ]
    }
   ],
   "source": [
    "# parameters to be tuned: \n",
    "\n",
    "# beta: discount factor for similarity, when common CMs for a pair of merchant is more than beta, no discount on similarity,\n",
    "# otherwise, discount the similarity result\n",
    "\n",
    "# the lower beta, the less discount to merchant similarity for merchants with less common CMs, \n",
    "# the more likely to include really small merchants to top k neighbors\n",
    "# the less # of CMs who have shopped in any of these neighbors, leading to less predicted tran_cnt in the validation/test data\n",
    "# on the other hand, if beta is too large, many merchants dimilarity will be discounted, end up with only nationally popular \n",
    "# merchants like Amazon in the top k neighbors\n",
    "\n",
    "# k:    only use the top k merchant neighbors to predict each CM's tran_cnt\n",
    "\n",
    "# initialize rmse and mae\n",
    "rmse = float('inf')\n",
    "mae  = float('inf')\n",
    "\n",
    "# best parameters based on rmse or mae as evaluation metrics\n",
    "best_beta = {'rmse': None, 'mae': None}\n",
    "best_k = {'rmse': None, 'mae': None}\n",
    "\n",
    "# begin parameter searching, get the best parameter combination based on rmse/mae on validation data set\n",
    "\n",
    "for beta in (10, 50, 100): \n",
    "    for k in (50, 100, 500):\n",
    "        print \"beta: %d; k: %d\" % (beta, k)\n",
    "        \n",
    "        # discount Cosine similarity based on discount factor beta\n",
    "        print \"compute Cosine similarity based on discount factor beta\"\n",
    "        strt_time = time.time()\n",
    "        \n",
    "        query = 'DROP TABLE IF EXISTS s_card_marketinganalytics.wfeng_5key_mrch_adj_sim'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        query = 'create table s_card_marketinganalytics.wfeng_5key_mrch_adj_sim as \\\n",
    "                 select mrch_brnd_nm, mrch_brnd_nm2, sim, num_acct, \\\n",
    "                 sim * least(num_acct, cast(' + str(beta) + ' as bigint)) / 1.0000 / ' + str(beta) + ' as adj_sim \\\n",
    "                 from s_card_marketinganalytics.wfeng_5key_mrch_sim'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "        \n",
    "        # get the rank of each merchant based on adj_sim\n",
    "        print \"get the rank of each merchant based on adj_sim\"\n",
    "        strt_time = time.time()\n",
    "        \n",
    "        query = 'DROP TABLE IF EXISTS s_card_marketinganalytics.wfeng_5key_mrch_adj_sim_rk'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        query = 'create table s_card_marketinganalytics.wfeng_5key_mrch_adj_sim_rk as \\\n",
    "                 select mrch_brnd_nm, mrch_brnd_nm2, sim, num_acct, adj_sim, rank() over(partition by mrch_brnd_nm order by adj_sim desc) as sim_rk \\\n",
    "                 from s_card_marketinganalytics.wfeng_5key_mrch_adj_sim'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "        \n",
    "        # make predicton based on the nearest k neighbors of each merchant\n",
    "        print \"make prediction based on the nearest k neighbors of each merchant\"\n",
    "        strt_time = time.time()\n",
    "        \n",
    "        query = 'drop table if exists s_card_marketinganalytics.wfeng_pred_based_on_adj_sim'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        query = \"\"\"\n",
    "                 create table s_card_marketinganalytics.wfeng_pred_based_on_adj_sim as \\\n",
    "                 select a.acct_key, a.mrch_brnd_nm, a.tran_cnt_cap, \\\n",
    "                 sum(b.tran_cnt_cap * c.adj_sim) / sum(c.adj_sim) as pred_tran_cnt \\\n",
    "                 from s_card_marketinganalytics.wfeng_validation_table a, \\\n",
    "                 s_card_marketinganalytics.wfeng_train_table b, \\\n",
    "                 s_card_marketinganalytics.wfeng_5key_mrch_adj_sim_rk c \\\n",
    "                 where a.acct_key = b.acct_key and a.mrch_brnd_nm <> b.mrch_brnd_nm \\\n",
    "                 and (a.mrch_brnd_nm in ('WAL-MART', 'AMAZON.COM', 'STARBUCKS', 'CVS') or a.mrch_brnd_nm like 'SAM%S CLUB') \\\n",
    "                 and a.mrch_brnd_nm = c.mrch_brnd_nm and b.mrch_brnd_nm = c.mrch_brnd_nm2 and c.sim_rk <= \"\"\" + str(k) + ' \\\n",
    "                 group by  a.acct_key, a.mrch_brnd_nm, a.tran_cnt_cap'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "        \n",
    "        # compute RMSE and MAE between observed and predicted values on validation data\n",
    "        print \"calculate RMSE and MAE\"\n",
    "        strt_time = time.time()\n",
    "        \n",
    "        query = 'select cast(sqrt(sum((b.tran_cnt_cap - a.pred_tran_cnt) * (b.tran_cnt_cap - a.pred_tran_cnt)) / count(*)) as decimal(18,4)) as rmse, \\\n",
    "                 cast(avg(abs(b.tran_cnt_cap - a.pred_tran_cnt)) as decimal(18,4)) as mae, \\\n",
    "                 count(*) as obs \\\n",
    "                 from s_card_marketinganalytics.wfeng_pred_based_on_adj_sim a \\\n",
    "                 inner join s_card_marketinganalytics.wfeng_validation_table b \\\n",
    "                 on a.acct_key = b.acct_key and a.mrch_brnd_nm = b.mrch_brnd_nm'\n",
    "\n",
    "        eval_validation = hiveContext.sql(query)\n",
    "        eval_pd = eval_validation.toPandas()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "\n",
    "        # parameter selection\n",
    "        if eval_pd['rmse'][0] < rmse:\n",
    "            rmse = eval_pd['rmse'][0]\n",
    "            best_beta['rmse'] = beta\n",
    "            best_k['rmse'] = k\n",
    "            \n",
    "        if eval_pd['mae'][0] < mae:\n",
    "            mae = eval_pd['mae'][0]\n",
    "            best_beta['mae'] = beta\n",
    "            best_k['mae'] = k\n",
    "\n",
    "print \"optimal rmse: %.2f; best beta: %d; best k: %d\" % (rmse, best_beta['rmse'], best_k['rmse'])\n",
    "print \"optimal mae: %.2f; best beta: %d; best k: %d\" % (mae, best_beta['mae'], best_k['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute Cosine similarity based on discount factor beta\n",
      "137 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "136 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "448 seconds\n",
      "calculate RMSE and MAE\n",
      "93 seconds\n"
     ]
    }
   ],
   "source": [
    "# use the optimal parameters to calculate rmse and mae on the test data set\n",
    "\n",
    "beta = best_beta['rmse']\n",
    "k = best_k['rmse']\n",
    "\n",
    "# discount Cosine similarity based on discount factor beta\n",
    "print \"compute Cosine similarity based on discount factor beta\"\n",
    "strt_time = time.time()\n",
    "\n",
    "query = 'DROP TABLE IF EXISTS s_card_marketinganalytics.wfeng_5key_mrch_adj_sim'\n",
    "hiveContext.sql(query)\n",
    "\n",
    "query = 'create table s_card_marketinganalytics.wfeng_5key_mrch_adj_sim as \\\n",
    "         select mrch_brnd_nm, mrch_brnd_nm2, sim, num_acct, \\\n",
    "         sim * least(num_acct, cast(' + str(beta) + ' as bigint)) / 1.0000 / ' + str(beta) + ' as adj_sim \\\n",
    "         from s_card_marketinganalytics.wfeng_5key_mrch_sim'\n",
    "hiveContext.sql(query)\n",
    "\n",
    "end_time = time.time()\n",
    "print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "\n",
    "# get the rank of each merchant based on adj_sim\n",
    "print \"get the rank of each merchant based on adj_sim\"\n",
    "strt_time = time.time()\n",
    "\n",
    "query = 'DROP TABLE IF EXISTS s_card_marketinganalytics.wfeng_5key_mrch_adj_sim_rk'\n",
    "hiveContext.sql(query)\n",
    "\n",
    "query = 'create table s_card_marketinganalytics.wfeng_5key_mrch_adj_sim_rk as \\\n",
    "         select mrch_brnd_nm, mrch_brnd_nm2, sim, num_acct, adj_sim, rank() over(partition by mrch_brnd_nm order by adj_sim desc) as sim_rk \\\n",
    "         from s_card_marketinganalytics.wfeng_5key_mrch_adj_sim'\n",
    "hiveContext.sql(query)\n",
    "\n",
    "end_time = time.time()\n",
    "print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "\n",
    "# make predicton based on the nearest k neighbors of each merchant\n",
    "print \"make prediction based on the nearest k neighbors of each merchant\"\n",
    "strt_time = time.time()\n",
    "\n",
    "query = 'drop table if exists s_card_marketinganalytics.wfeng_pred_based_on_adj_sim'\n",
    "hiveContext.sql(query)\n",
    "\n",
    "query = \"\"\"\n",
    "         create table s_card_marketinganalytics.wfeng_pred_based_on_adj_sim as \\\n",
    "         select a.acct_key, a.mrch_brnd_nm, a.tran_cnt_cap, \\\n",
    "         sum(b.tran_cnt_cap * c.adj_sim) / sum(c.adj_sim) as pred_tran_cnt \\\n",
    "         from s_card_marketinganalytics.wfeng_test_table a, \\\n",
    "         s_card_marketinganalytics.wfeng_train_table b, \\\n",
    "         s_card_marketinganalytics.wfeng_5key_mrch_adj_sim_rk c \\\n",
    "         where a.acct_key = b.acct_key and a.mrch_brnd_nm <> b.mrch_brnd_nm \\\n",
    "         and (a.mrch_brnd_nm in ('WAL-MART', 'AMAZON.COM', 'STARBUCKS', 'CVS') or a.mrch_brnd_nm like 'SAM%S CLUB') \\\n",
    "         and a.mrch_brnd_nm = c.mrch_brnd_nm and b.mrch_brnd_nm = c.mrch_brnd_nm2 and c.sim_rk <= \"\"\" + str(k) + ' \\\n",
    "         group by  a.acct_key, a.mrch_brnd_nm, a.tran_cnt_cap'\n",
    "hiveContext.sql(query)\n",
    "\n",
    "query = 'select * from s_card_marketinganalytics.wfeng_pred_based_on_adj_sim'\n",
    "pred = hiveContext.sql(query)\n",
    "\n",
    "end_time = time.time()\n",
    "print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "\n",
    "# compute RMSE and MAE between observed and predicted values on validation data\n",
    "print \"calculate RMSE and MAE\"\n",
    "strt_time = time.time()\n",
    "\n",
    "query = 'select cast(sqrt(sum((b.tran_cnt_cap - a.pred_tran_cnt) * (b.tran_cnt_cap - a.pred_tran_cnt)) / count(*)) as decimal(18,4)) as rmse, \\\n",
    "         cast(avg(abs(b.tran_cnt_cap - a.pred_tran_cnt)) as decimal(18,4)) as mae, \\\n",
    "         count(*) as obs \\\n",
    "         from s_card_marketinganalytics.wfeng_pred_based_on_adj_sim a \\\n",
    "         inner join s_card_marketinganalytics.wfeng_test_table b \\\n",
    "         on a.acct_key = b.acct_key and a.mrch_brnd_nm = b.mrch_brnd_nm'\n",
    "\n",
    "eval_validation = hiveContext.sql(query)\n",
    "test_eval_pd = eval_validation.toPandas()\n",
    "\n",
    "end_time = time.time()\n",
    "print\"{0:.0f} seconds\" .format(end_time - strt_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+-------------+\n",
      "|  acct_key|mrch_brnd_nm|tran_cnt_cap|pred_tran_cnt|\n",
      "+----------+------------+------------+-------------+\n",
      "|   3565152|  AMAZON.COM|           8|          1.0|\n",
      "|  26928774|         CVS|           3|          1.0|\n",
      "|1008425567|         CVS|           1|          1.0|\n",
      "|1031290378|   STARBUCKS|           5|          1.0|\n",
      "|1022660936|    WAL-MART|           5|          1.0|\n",
      "|1040082821|         CVS|           1|          2.0|\n",
      "|1018396825|  SAM'S CLUB|          10|          1.0|\n",
      "|1033991287|   STARBUCKS|           1|          1.0|\n",
      "|1034924138|   STARBUCKS|           1|          1.0|\n",
      "|1034449067|    WAL-MART|          18|          1.0|\n",
      "+----------+------------+------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.8331</td>\n",
       "      <td>3.4624</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rmse     mae  obs\n",
       "0  5.8331  3.4624  625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eval_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized RMSE: 0.32\n",
      "normalized MAE: 0.19\n"
     ]
    }
   ],
   "source": [
    "# to compare the results between the raw cosine similarity measure and the adjusted similarity measure\n",
    "# normalize the rmse/mae against y_max - y_min\n",
    "# https://ipfs.io/ipfs/QmXoypizjW3WknFiJnKLwHCnL72vedxjQkDDP1mXWo6uco/wiki/Root_mean_square_error.html\n",
    "# in raw cosine simialrity, y_max = 18, y_min = 0;\n",
    "\n",
    "print \"normalized RMSE: %.2f\" % float(test_eval_pd['rmse']/(18-0))\n",
    "print \"normalized MAE: %.2f\" % float(test_eval_pd['mae']/(18-0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 10, 'rmse': 10}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 100, 'rmse': 100}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Select merchant neighbors based on Adjusted Cosine similarity: transform tran_cnt by merchant (column) scaling and acct_key (row) centering **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 ms, sys: 2 ms, total: 3 ms\n",
      "Wall time: 6.17 s\n"
     ]
    }
   ],
   "source": [
    "# scale/normalize tran_cnt over each merchant column (divide each tran_cnt by the max tran_cnt in each merchant column) to\n",
    "# make every merchant column has the same scale [0,1]\n",
    "\n",
    "query = \"\"\"\n",
    "        select acct_key, mrch_brnd_nm, mrch_adj_tran - avg(mrch_adj_tran) over(partition by acct_key) as acct_adj_tran\n",
    "        from \n",
    "        (select acct_key, mrch_brnd_nm, \n",
    "        cast(tran_cnt_cap as float) / max(tran_cnt_cap) over(partition by mrch_brnd_nm) as mrch_adj_tran\n",
    "        from s_card_marketinganalytics.wfeng_acct_tran_cap) t\n",
    "        \"\"\"\n",
    "\n",
    "%time df = hiveContext.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train2, validation2, test2 = df.randomSplit([0.8,0.1,0.1], seed=1) # make sure we get the same splits every time we run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hiveContext.registerDataFrameAsTable(train2,'train2_table')\n",
    "hiveContext.registerDataFrameAsTable(validation2,'validation2_table')\n",
    "hiveContext.registerDataFrameAsTable(test2,'test2_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 µs, sys: 1 ms, total: 2 ms\n",
      "Wall time: 3.83 s\n",
      "CPU times: user 199 ms, sys: 54 ms, total: 253 ms\n",
      "Wall time: 27min 39s\n",
      "CPU times: user 0 ns, sys: 2 ms, total: 2 ms\n",
      "Wall time: 1.27 s\n",
      "CPU times: user 138 ms, sys: 97 ms, total: 235 ms\n",
      "Wall time: 26min 55s\n",
      "CPU times: user 2 ms, sys: 1 ms, total: 3 ms\n",
      "Wall time: 3.67 s\n",
      "CPU times: user 147 ms, sys: 43 ms, total: 190 ms\n",
      "Wall time: 20min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save split tables to Hive\n",
    "%time hiveContext.sql('drop table if exists s_card_marketinganalytics.wfeng_train2_table')\n",
    "%time hiveContext.sql('create table s_card_marketinganalytics.wfeng_train2_table as select * from train2_table')\n",
    "\n",
    "%time hiveContext.sql('drop table if exists s_card_marketinganalytics.wfeng_validation2_table')\n",
    "%time hiveContext.sql('create table s_card_marketinganalytics.wfeng_validation2_table as select * from validation2_table')\n",
    "\n",
    "%time hiveContext.sql('drop table if exists s_card_marketinganalytics.wfeng_test2_table')\n",
    "%time hiveContext.sql('create table s_card_marketinganalytics.wfeng_test2_table as select * from test2_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 µs, sys: 1 ms, total: 2 ms\n",
      "Wall time: 5.37 s\n",
      "CPU times: user 76 ms, sys: 23 ms, total: 99 ms\n",
      "Wall time: 10min 49s\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 13.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[mrch_brnd_nm: string, mrch_brnd_nm2: string, sim: double, num_acct: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Adjusted Cosine similarity between each 5 key merchants and all other merchants\n",
    "query = 'drop table if exists s_card_marketinganalytics.wfeng_5key_mrch_sim2'\n",
    "%time hiveContext.sql(query)\n",
    "\n",
    "query = \"\"\"\n",
    "        CREATE TABLE s_card_marketinganalytics.wfeng_5key_mrch_sim2 AS\n",
    "        SELECT a.mrch_brnd_nm, b.mrch_brnd_nm AS mrch_brnd_nm2,\n",
    "        SUM(a.acct_adj_tran * b.acct_adj_tran)/(SQRT(SUM(a.acct_adj_tran * a.acct_adj_tran)) * SQRT(SUM(b.acct_adj_tran * b.acct_adj_tran))) AS sim\n",
    "        , COUNT(*) AS num_acct\n",
    "        FROM s_card_marketinganalytics.wfeng_train2_table a\n",
    "        inner JOIN s_card_marketinganalytics.wfeng_train2_table b\n",
    "        ON a.acct_key = b.acct_key\n",
    "        WHERE a.mrch_brnd_nm in ('WAL-MART', 'AMAZON.COM', 'STARBUCKS', 'CVS') or a.mrch_brnd_nm like 'SAM%S CLUB'\n",
    "        GROUP BY a.mrch_brnd_nm, b.mrch_brnd_nm\n",
    "        \"\"\"\n",
    "%time hiveContext.sql(query)\n",
    "\n",
    "query = 'select * from s_card_marketinganalytics.wfeng_5key_mrch_sim2'\n",
    "%time hiveContext.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 50; k: 50\n",
      "compute Cosine similarity based on discount factor beta\n",
      "142 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "147 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "488 seconds\n",
      "calculate RMSE and MAE\n",
      "101 seconds\n",
      "beta: 50; k: 100\n",
      "compute Cosine similarity based on discount factor beta\n",
      "145 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "145 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "542 seconds\n",
      "calculate RMSE and MAE\n",
      "107 seconds\n",
      "beta: 50; k: 500\n",
      "compute Cosine similarity based on discount factor beta\n",
      "106 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "118 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "483 seconds\n",
      "calculate RMSE and MAE\n",
      "108 seconds\n",
      "beta: 100; k: 50\n",
      "compute Cosine similarity based on discount factor beta\n",
      "123 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "122 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "504 seconds\n",
      "calculate RMSE and MAE\n",
      "98 seconds\n",
      "beta: 100; k: 100\n",
      "compute Cosine similarity based on discount factor beta\n",
      "116 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "139 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "501 seconds\n",
      "calculate RMSE and MAE\n",
      "108 seconds\n",
      "beta: 100; k: 500\n",
      "compute Cosine similarity based on discount factor beta\n",
      "98 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "128 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "497 seconds\n",
      "calculate RMSE and MAE\n",
      "115 seconds\n",
      "beta: 500; k: 50\n",
      "compute Cosine similarity based on discount factor beta\n",
      "104 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "126 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "473 seconds\n",
      "calculate RMSE and MAE\n",
      "115 seconds\n",
      "beta: 500; k: 100\n",
      "compute Cosine similarity based on discount factor beta\n",
      "134 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "130 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "460 seconds\n",
      "calculate RMSE and MAE\n",
      "100 seconds\n",
      "beta: 500; k: 500\n",
      "compute Cosine similarity based on discount factor beta\n",
      "116 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "151 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "501 seconds\n",
      "calculate RMSE and MAE\n",
      "141 seconds\n",
      "optimal rmse: 0.32; best beta: 500; best k: 500\n",
      "optimal mae: 0.22; best beta: 500; best k: 500\n"
     ]
    }
   ],
   "source": [
    "# parameters to be tuned: \n",
    "\n",
    "# beta: discount factor for similarity, when common CMs for a pair of merchant is more than beta, no discount on similarity,\n",
    "# otherwise, discount the similarity result\n",
    "\n",
    "# the lower beta, the less discount to merchant similarity for merchants with less common CMs, \n",
    "# the more likely to include really small merchants to top k neighbors\n",
    "# the less # of CMs who have shopped in any of these neighbors, leading to less predicted tran_cnt in the validation/test data\n",
    "# on the other hand, if beta is too large, many merchants dimilarity will be discounted, end up with only nationally popular \n",
    "# merchants like Amazon in the top k neighbors\n",
    "\n",
    "# initialize rmse and mae\n",
    "rmse = float('inf')\n",
    "mae  = float('inf')\n",
    "\n",
    "# best parameters based on rmse or mae as evaluation metrics\n",
    "best_beta2 = {'rmse': None, 'mae': None}\n",
    "best_k2 = {'rmse': None, 'mae': None}\n",
    "\n",
    "# begin parameter searching, get the best parameter combination based on rmse/mae on validation data set\n",
    "for beta in (50, 100, 500):\n",
    "    for k in (50, 100, 500):\n",
    "        print \"beta: %d; k: %d\" % (beta, k)\n",
    "        \n",
    "        # discount Cosine similarity based on discount factor beta\n",
    "        print \"compute Cosine similarity based on discount factor beta\"\n",
    "        strt_time = time.time()\n",
    "        \n",
    "        query = 'DROP TABLE IF EXISTS s_card_marketinganalytics.wfeng_5key_mrch_adj_sim2'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        query = 'create table s_card_marketinganalytics.wfeng_5key_mrch_adj_sim2 as \\\n",
    "                 select mrch_brnd_nm, mrch_brnd_nm2, sim, num_acct, \\\n",
    "                 sim * least(num_acct, cast(' + str(beta) + ' as bigint)) / 1.0000 / ' + str(beta) + ' as adj_sim \\\n",
    "                 from s_card_marketinganalytics.wfeng_5key_mrch_sim2'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "        \n",
    "        # get the rank of each merchant based on adj_sim\n",
    "        print \"get the rank of each merchant based on adj_sim\"\n",
    "        strt_time = time.time()\n",
    "        \n",
    "        query = 'DROP TABLE IF EXISTS s_card_marketinganalytics.wfeng_5key_mrch_adj_sim_rk2'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        query = 'create table s_card_marketinganalytics.wfeng_5key_mrch_adj_sim_rk2 as \\\n",
    "                 select mrch_brnd_nm, mrch_brnd_nm2, sim, num_acct, adj_sim, rank() over(partition by mrch_brnd_nm order by adj_sim desc) as sim_rk \\\n",
    "                 from s_card_marketinganalytics.wfeng_5key_mrch_adj_sim2'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "        \n",
    "        # make predicton based on the nearest k neighbors of each merchant\n",
    "        print \"make prediction based on the nearest k neighbors of each merchant\"\n",
    "        strt_time = time.time()\n",
    "        \n",
    "        query = 'drop table if exists s_card_marketinganalytics.wfeng_pred_based_on_adj_sim2'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        query = \"\"\"\n",
    "                 create table s_card_marketinganalytics.wfeng_pred_based_on_adj_sim2 as \\\n",
    "                 select a.acct_key, a.mrch_brnd_nm, a.acct_adj_tran, \\\n",
    "                 sum(b.acct_adj_tran * c.adj_sim) / sum(c.adj_sim) as pred_tran_cnt \\\n",
    "                 from s_card_marketinganalytics.wfeng_validation2_table a, \\\n",
    "                 s_card_marketinganalytics.wfeng_train2_table b, \\\n",
    "                 s_card_marketinganalytics.wfeng_5key_mrch_adj_sim_rk2 c \\\n",
    "                 where a.acct_key = b.acct_key and a.mrch_brnd_nm <> b.mrch_brnd_nm \\\n",
    "                 and (a.mrch_brnd_nm in ('WAL-MART', 'AMAZON.COM', 'STARBUCKS', 'CVS') or a.mrch_brnd_nm like 'SAM%S CLUB') \\\n",
    "                 and a.mrch_brnd_nm = c.mrch_brnd_nm and b.mrch_brnd_nm = c.mrch_brnd_nm2 and c.sim_rk <= \"\"\" + str(k) + ' \\\n",
    "                 group by  a.acct_key, a.mrch_brnd_nm, a.acct_adj_tran'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "        \n",
    "        # compute RMSE and MAE between observed and predicted values on validation data\n",
    "        print \"calculate RMSE and MAE\"\n",
    "        strt_time = time.time()\n",
    "        \n",
    "        query = 'select cast(sqrt(sum((b.acct_adj_tran - a.pred_tran_cnt) * (b.acct_adj_tran - a.pred_tran_cnt)) / count(*)) as decimal(18,4)) as rmse, \\\n",
    "                 cast(avg(abs(b.acct_adj_tran - a.pred_tran_cnt)) as decimal(18,4)) as mae, \\\n",
    "                 count(*) as obs \\\n",
    "                 from s_card_marketinganalytics.wfeng_pred_based_on_adj_sim2 a \\\n",
    "                 inner join s_card_marketinganalytics.wfeng_validation2_table b \\\n",
    "                 on a.acct_key = b.acct_key and a.mrch_brnd_nm = b.mrch_brnd_nm'\n",
    "\n",
    "        eval_validation = hiveContext.sql(query)\n",
    "        eval_pd = eval_validation.toPandas()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "\n",
    "        # parameter selection\n",
    "        if eval_pd['rmse'][0] < rmse:\n",
    "            rmse = eval_pd['rmse'][0]\n",
    "            best_beta2['rmse'] = beta\n",
    "            best_k2['rmse'] = k\n",
    "            \n",
    "        if eval_pd['mae'][0] < mae:\n",
    "            mae = eval_pd['mae'][0]\n",
    "            best_beta2['mae'] = beta\n",
    "            best_k2['mae'] = k\n",
    "\n",
    "print \"optimal rmse: %.2f; best beta: %d; best k: %d\" % (rmse, best_beta2['rmse'], best_k2['rmse'])\n",
    "print \"optimal mae: %.2f; best beta: %d; best k: %d\" % (mae, best_beta2['mae'], best_k2['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute Cosine similarity based on discount factor beta\n",
      "127 seconds\n",
      "get the rank of each merchant based on adj_sim\n",
      "137 seconds\n",
      "make prediction based on the nearest k neighbors of each merchant\n",
      "527 seconds\n",
      "calculate RMSE and MAE\n",
      "146 seconds\n"
     ]
    }
   ],
   "source": [
    "# use the optimal parameters to calculate rmse and mae on the test data set\n",
    "\n",
    "# discount Cosine similarity based on discount factor beta\n",
    "print \"compute Cosine similarity based on discount factor beta\"\n",
    "strt_time = time.time()\n",
    "\n",
    "query = 'DROP TABLE IF EXISTS s_card_marketinganalytics.wfeng_5key_mrch_adj_sim2'\n",
    "hiveContext.sql(query)\n",
    "\n",
    "query = 'create table s_card_marketinganalytics.wfeng_5key_mrch_adj_sim2 as \\\n",
    "         select mrch_brnd_nm, mrch_brnd_nm2, sim, num_acct, \\\n",
    "         sim * least(num_acct, cast(' + str(beta) + ' as bigint)) / 1.0000 / ' + str(beta) + ' as adj_sim \\\n",
    "         from s_card_marketinganalytics.wfeng_5key_mrch_sim2'\n",
    "hiveContext.sql(query)\n",
    "\n",
    "end_time = time.time()\n",
    "print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "\n",
    "# get the rank of each merchant based on adj_sim\n",
    "print \"get the rank of each merchant based on adj_sim\"\n",
    "strt_time = time.time()\n",
    "\n",
    "query = 'DROP TABLE IF EXISTS s_card_marketinganalytics.wfeng_5key_mrch_adj_sim_rk2'\n",
    "hiveContext.sql(query)\n",
    "\n",
    "query = 'create table s_card_marketinganalytics.wfeng_5key_mrch_adj_sim_rk2 as \\\n",
    "         select mrch_brnd_nm, mrch_brnd_nm2, sim, num_acct, adj_sim, rank() over(partition by mrch_brnd_nm order by adj_sim desc) as sim_rk \\\n",
    "         from s_card_marketinganalytics.wfeng_5key_mrch_adj_sim2'\n",
    "hiveContext.sql(query)\n",
    "\n",
    "end_time = time.time()\n",
    "print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "\n",
    "# make predicton based on the nearest k neighbors of each merchant\n",
    "print \"make prediction based on the nearest k neighbors of each merchant\"\n",
    "strt_time = time.time()\n",
    "\n",
    "query = 'drop table if exists s_card_marketinganalytics.wfeng_pred_based_on_adj_sim2'\n",
    "hiveContext.sql(query)\n",
    "\n",
    "query = \"\"\"\n",
    "         create table s_card_marketinganalytics.wfeng_pred_based_on_adj_sim2 as \\\n",
    "         select a.acct_key, a.mrch_brnd_nm, a.acct_adj_tran, \\\n",
    "         sum(b.acct_adj_tran * c.adj_sim) / sum(c.adj_sim) as pred_tran_cnt \\\n",
    "         from s_card_marketinganalytics.wfeng_test2_table a, \\\n",
    "         s_card_marketinganalytics.wfeng_train2_table b, \\\n",
    "         s_card_marketinganalytics.wfeng_5key_mrch_adj_sim_rk2 c \\\n",
    "         where a.acct_key = b.acct_key and a.mrch_brnd_nm <> b.mrch_brnd_nm \\\n",
    "         and (a.mrch_brnd_nm in ('WAL-MART', 'AMAZON.COM', 'STARBUCKS', 'CVS') or a.mrch_brnd_nm like 'SAM%S CLUB') \\\n",
    "         and a.mrch_brnd_nm = c.mrch_brnd_nm and b.mrch_brnd_nm = c.mrch_brnd_nm2 and c.sim_rk <= \"\"\" + str(k) + ' \\\n",
    "         group by  a.acct_key, a.mrch_brnd_nm, a.acct_adj_tran'\n",
    "hiveContext.sql(query)\n",
    "\n",
    "query = 'select * from s_card_marketinganalytics.wfeng_pred_based_on_adj_sim2'\n",
    "pred2 = hiveContext.sql(query)\n",
    "\n",
    "end_time = time.time()\n",
    "print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "\n",
    "# compute RMSE and MAE between observed and predicted values on validation data\n",
    "print \"calculate RMSE and MAE\"\n",
    "strt_time = time.time()\n",
    "\n",
    "query = 'select cast(sqrt(sum((b.acct_adj_tran - a.pred_tran_cnt) * (b.acct_adj_tran - a.pred_tran_cnt)) / count(*)) as decimal(18,4)) as rmse, \\\n",
    "         cast(avg(abs(b.acct_adj_tran - a.pred_tran_cnt)) as decimal(18,4)) as mae, \\\n",
    "         count(*) as obs \\\n",
    "         from s_card_marketinganalytics.wfeng_pred_based_on_adj_sim2 a \\\n",
    "         inner join s_card_marketinganalytics.wfeng_test2_table b \\\n",
    "         on a.acct_key = b.acct_key and a.mrch_brnd_nm = b.mrch_brnd_nm'\n",
    "\n",
    "eval_validation = hiveContext.sql(query)\n",
    "test2_eval_pd = eval_validation.toPandas()\n",
    "\n",
    "end_time = time.time()\n",
    "print\"{0:.0f} seconds\" .format(end_time - strt_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+--------------------+\n",
      "|  acct_key|mrch_brnd_nm|acct_adj_tran|       pred_tran_cnt|\n",
      "+----------+------------+-------------+--------------------+\n",
      "|  55844029|  AMAZON.COM|    0.7912342| 0.04123418405652046|\n",
      "|  62456902|  SAM'S CLUB|   0.35086823|  0.3681243298743516|\n",
      "|1039331228|    WAL-MART| -0.111278675| 0.16649910807609558|\n",
      "|1041753023|    WAL-MART| -0.012747193| 0.20947502553462982|\n",
      "|  47330235|    WAL-MART|    0.8582757|  0.1402357225219253|\n",
      "|1040980661|    WAL-MART|    0.5735618|  0.5180062055587769|\n",
      "|1039753690|  AMAZON.COM|   0.18123867| 0.04234977811574936|\n",
      "|1023432227|         CVS|  -0.08402885|-0.06266132742166519|\n",
      "|  41540063|   STARBUCKS|  0.010241284|-0.04531427100300788|\n",
      "|  26580914|  AMAZON.COM|-0.0077978163| 0.49220219254493713|\n",
      "+----------+------------+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3156</td>\n",
       "      <td>0.2167</td>\n",
       "      <td>388236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rmse     mae     obs\n",
       "0  0.3156  0.2167  388236"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2_eval_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized RMSE: 0.32\n",
      "normalized MAE: 0.22\n"
     ]
    }
   ],
   "source": [
    "# to compare with the raw cosine similarity measure, normalize the rmse/mae against y_max - y_min\n",
    "# https://ipfs.io/ipfs/QmXoypizjW3WknFiJnKLwHCnL72vedxjQkDDP1mXWo6uco/wiki/Root_mean_square_error.html\n",
    "# in adjusted cosine similarity, y_max = 0.5, ,y_min = -0.5\n",
    "\n",
    "print \"normalized RMSE: %.2f\" % float(test2_eval_pd['rmse'])\n",
    "print \"normalized MAE: %.2f\" % float(test2_eval_pd['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 500, 'rmse': 500}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_beta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': 500, 'rmse': 500}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Use Rank-based evaluation metrics, so we have to compute similarity between each pair of merchants **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 ms, sys: 999 µs, total: 2 ms\n",
      "Wall time: 4.12 s\n",
      "CPU times: user 1.17 s, sys: 324 ms, total: 1.49 s\n",
      "Wall time: 2h 44min 11s\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 12.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[mrch_brnd_nm: string, mrch_brnd_nm2: string, sim: double, num_acct: bigint]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create raw Cosine similarity between each pair of merchants\n",
    "query = 'drop table if exists s_card_marketinganalytics.wfeng_mrch_sim'\n",
    "%time hiveContext.sql(query)\n",
    "\n",
    "query = \"\"\"\n",
    "        CREATE TABLE s_card_marketinganalytics.wfeng_mrch_sim AS\n",
    "        SELECT a.mrch_brnd_nm, b.mrch_brnd_nm AS mrch_brnd_nm2,\n",
    "        SUM(a.tran_cnt_cap * b.tran_cnt_cap)/(SQRT(SUM(a.tran_cnt_cap * a.tran_cnt_cap)) * SQRT(SUM(b.tran_cnt_cap * b.tran_cnt_cap))) AS sim\n",
    "        , COUNT(*) AS num_acct\n",
    "        FROM s_card_marketinganalytics.wfeng_train_table a\n",
    "        inner JOIN s_card_marketinganalytics.wfeng_train_table b\n",
    "        ON a.acct_key = b.acct_key\n",
    "        GROUP BY a.mrch_brnd_nm, b.mrch_brnd_nm\n",
    "        \"\"\"\n",
    "%time hiveContext.sql(query)\n",
    "\n",
    "query = 'select * from s_card_marketinganalytics.wfeng_mrch_sim'\n",
    "%time hiveContext.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 ms, sys: 999 µs, total: 4 ms\n",
      "Wall time: 8.76 s\n",
      "CPU times: user 1.08 s, sys: 249 ms, total: 1.33 s\n",
      "Wall time: 2h 26min 1s\n",
      "CPU times: user 2 ms, sys: 2 ms, total: 4 ms\n",
      "Wall time: 10.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[mrch_brnd_nm: string, mrch_brnd_nm2: string, sim: double, num_acct: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Adjusted Cosine similarity between each pair of merchants\n",
    "query = 'drop table if exists s_card_marketinganalytics.wfeng_mrch_sim2'\n",
    "%time hiveContext.sql(query)\n",
    "\n",
    "query = \"\"\"\n",
    "        CREATE TABLE s_card_marketinganalytics.wfeng_mrch_sim2 AS\n",
    "        SELECT a.mrch_brnd_nm, b.mrch_brnd_nm AS mrch_brnd_nm2,\n",
    "        SUM(a.acct_adj_tran * b.acct_adj_tran)/(SQRT(SUM(a.acct_adj_tran * a.acct_adj_tran)) * SQRT(SUM(b.acct_adj_tran * b.acct_adj_tran))) AS sim\n",
    "        , COUNT(*) AS num_acct\n",
    "        FROM s_card_marketinganalytics.wfeng_train2_table a\n",
    "        inner JOIN s_card_marketinganalytics.wfeng_train2_table b\n",
    "        ON a.acct_key = b.acct_key\n",
    "        GROUP BY a.mrch_brnd_nm, b.mrch_brnd_nm\n",
    "        \"\"\"\n",
    "%time hiveContext.sql(query)\n",
    "\n",
    "query = 'select * from s_card_marketinganalytics.wfeng_mrch_sim2'\n",
    "%time hiveContext.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from scipy.stats import spearmanr\n",
    "from  pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "\n",
    "def spearman_corr(x,y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    if len(x) != len(y):\n",
    "        return float(2.0)\n",
    "    return float(spearmanr(x,y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: 50; k: 100\n",
      "compute Cosine similarity based on discount factor beta\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hiveContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ab727a81438f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DROP TABLE IF EXISTS s_card_marketinganalytics.wfeng_mrch_adj_sim'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mhiveContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'create table s_card_marketinganalytics.wfeng_mrch_adj_sim as                  select mrch_brnd_nm, mrch_brnd_nm2, sim, num_acct,                  sim * least(num_acct, cast('\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' as bigint)) / 1.0000 / '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' as adj_sim                  from s_card_marketinganalytics.wfeng_mrch_sim'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hiveContext' is not defined"
     ]
    }
   ],
   "source": [
    "# parameters to be tuned: \n",
    "\n",
    "# beta: discount factor for similarity, when common CMs for a pair of merchant is more than beta, no discount on similarity,\n",
    "        # otherwise, discount the similarity result\n",
    "# k:    only use the top k merchant neighbors to predict each CM's tran_cnt\n",
    "\n",
    "# initialize rmse and mae\n",
    "spearman_corr = 0.0\n",
    "\n",
    "# best parameters based on rmse or mae as evaluation metrics\n",
    "best_beta3 = None\n",
    "best_k3 = None\n",
    "\n",
    "# begin parameter searching, get the best parameter combination based on rmse/mae on validation data set\n",
    "\n",
    "for beta in (50, 500): \n",
    "    for k in (100, 500):\n",
    "        print \"beta: %d; k: %d\" % (beta, k)\n",
    "        \n",
    "        # discount Cosine similarity based on discount factor beta\n",
    "        print \"compute Cosine similarity based on discount factor beta\"\n",
    "        strt_time = time.time()\n",
    "        \n",
    "        query = 'DROP TABLE IF EXISTS s_card_marketinganalytics.wfeng_mrch_adj_sim'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        query = 'create table s_card_marketinganalytics.wfeng_mrch_adj_sim as \\\n",
    "                 select mrch_brnd_nm, mrch_brnd_nm2, sim, num_acct, \\\n",
    "                 sim * least(num_acct, cast(' + str(beta) + ' as bigint)) / 1.0000 / ' + str(beta) + ' as adj_sim \\\n",
    "                 from s_card_marketinganalytics.wfeng_mrch_sim'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "        \n",
    "        # get the rank of each merchant based on adj_sim\n",
    "        print \"get the rank of each merchant based on adj_sim\"\n",
    "        strt_time = time.time()\n",
    "        \n",
    "        query = 'DROP TABLE IF EXISTS s_card_marketinganalytics.wfeng_mrch_adj_sim_rk'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        query = 'create table s_card_marketinganalytics.wfeng_mrch_adj_sim_rk as \\\n",
    "                 select mrch_brnd_nm, mrch_brnd_nm2, sim, num_acct, adj_sim, rank() over(partition by mrch_brnd_nm order by adj_sim desc) as sim_rk \\\n",
    "                 from s_card_marketinganalytics.wfeng_mrch_adj_sim'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "        \n",
    "        # make predicton based on the nearest k neighbors of each merchant\n",
    "        print \"make prediction based on the nearest k neighbors of each merchant\"\n",
    "        strt_time = time.time()\n",
    "        \n",
    "        query = 'drop table if exists s_card_marketinganalytics.wfeng_pred_based_on_adj_sim3'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        query = \"\"\"\n",
    "                 create table s_card_marketinganalytics.wfeng_pred_based_on_adj_sim3 as \\\n",
    "                 select a.acct_key, a.mrch_brnd_nm, a.tran_cnt_cap, \\\n",
    "                 sum(b.tran_cnt_cap * c.adj_sim) / sum(c.adj_sim) as pred_tran_cnt \\\n",
    "                 from s_card_marketinganalytics.wfeng_validation_table a, \\\n",
    "                 s_card_marketinganalytics.wfeng_train_table b, \\\n",
    "                 s_card_marketinganalytics.wfeng_mrch_adj_sim_rk c \\\n",
    "                 where a.acct_key = b.acct_key and a.mrch_brnd_nm <> b.mrch_brnd_nm \\\n",
    "                 and a.mrch_brnd_nm = c.mrch_brnd_nm and b.mrch_brnd_nm = c.mrch_brnd_nm2 and c.sim_rk <= \"\"\" + str(k) + ' \\\n",
    "                 group by  a.acct_key, a.mrch_brnd_nm, a.tran_cnt_cap'\n",
    "        hiveContext.sql(query)\n",
    "        \n",
    "        # include training observation into the ranking to get more reliable rank\n",
    "        query = \"\"\"\n",
    "                SELECT acct_key, mrch_brnd_nm, tran_cnt_cap AS obs, tran_cnt_cap AS pred\n",
    "                FROM s_card_marketinganalytics.wfeng_train_table\n",
    "                UNION\n",
    "                SELECT acct_key, mrch_brnd_nm, tran_cnt_cap AS obs, pred_tran_cnt AS pred\n",
    "                FROM s_card_marketinganalytics.wfeng_pred_based_on_adj_sim\n",
    "                \"\"\"\n",
    "        pred3 = hiveContext.sql(query)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "        \n",
    "        # compute Speaman ranking coefficient between observed and predicted values on the union of train and validation data\n",
    "        print \"calculate ranking evaluation metric\"\n",
    "        strt_time = time.time()\n",
    "        \n",
    "        temp = pred3.groupBy('acct_key').agg(f.collect_list(pred['obs']).alias('obs'), \n",
    "                                             f.collect_list(pred['pred']).alias('pred'))\n",
    "        \n",
    "        pred3_rk = temp.select('acct_key',f.UserDefinedFunction(spearman_corr,DoubleType())('obs','pred'))\n",
    "        \n",
    "        pred3_rk = pred3_rk.withColumnRenamed('spearman_corr(obs, pred)', 'spearman_corr')\n",
    "        \n",
    "        pred3_rk.registerTempTable('pred3_rk')\n",
    "        hiveContext.sql('create table s_card_marketinganalytics.spearman_corr_raw as select * from pred3_rk')\n",
    "\n",
    "        eval_ = hiveContext.sql('''\n",
    "                                select avg(spearman_corr) as spearman_corr\n",
    "                                from s_card_marketinganalytics.spearman_corr_raw\n",
    "                                where spearman_corr <> 'NaN' and spearman_corr is not null\n",
    "                                ''')\n",
    "        eval_pd = eval_.toPandas()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print\"{0:.0f} seconds\" .format(end_time - strt_time)\n",
    "\n",
    "        # parameter selection\n",
    "        if eval_pd['spearman_corr'][0] > spearman_corr:\n",
    "            spearman_corr = eval_pd['spearman_corr'][0]\n",
    "            best_beta3 = beta\n",
    "            best_k3 = k\n",
    "\n",
    "print \"optimal spearman_corr: %.2f; best beta: %d; best k: %d\" % (spearman_corr, best_beta3, best_k3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred3_rk.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
