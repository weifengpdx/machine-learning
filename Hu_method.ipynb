{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Basic  system Set up\n",
    "'''\n",
    "import time\n",
    "from time import gmtime, strftime \n",
    "timetmp = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n",
    "appname = 'Pyspark_' + timetmp\n",
    "\n",
    "import os\n",
    "## update the pyspark config\n",
    "os.environ[\"LD_LIBRARY_PATH\"]=\"/opt/rh/python27/root/usr/lib64\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/dsap/devl/private/common/python2.7/bin/python\"\n",
    "#”/usr/hdp/current/spark2-client” is spark2\n",
    "os.environ[\"SPARK_HOME\"]=\"/usr/hdp/current/spark2-client\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/dsap/devl/private/common/python2.7/bin/jupyter\"\n",
    "os.environ[\"PYTHONPATH\"]=\"/usr/hdp/current/spark2-client/python:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip\"\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf=(SparkConf().setMaster(\"yarn-client\").setAppName(appname)\n",
    "      .set(\"spark.yarn.queue\", \"BIA-support\")\n",
    "      .set(\"spark.num.executors\",\"20\")\n",
    "      .set(\"spark.driver.cores\",\"8\") # not sure if this can help improve speed\n",
    "      .set(\"spark.executor.memory\", \"20g\")\n",
    "     #.set(\"spark.executor.instances\",\"20\") # not sure if this can help improve speed\n",
    "     #.set(\"spark.shuffle.service.enabled\",\"true\") # not sure if this can help improve speed\n",
    "     #.set(\"spark.dynamicAllocation.enabled\",\"true\") # not sure if this can help improve speed\n",
    "     #.set(\"spark.dynamicAllocation.minExecutors\",\"5\") # not sure if this can help improve speed\n",
    "     )\n",
    "\n",
    "sc=SparkContext(conf=conf)\n",
    "from pyspark.sql import HiveContext\n",
    "hiveContext=HiveContext(sc) \n",
    "\n",
    "print\"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** import data with missing values filled with 0s **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only use the top 100 merchants\n",
    "get_query = 'select acct_key, mrch_brnd_id, tran_cnt from s_card_marketinganalytics.wfeng_top100_mrch_full'\n",
    "dataframe = hiveContext.sql(get_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = dataframe.randomSplit([0.8,0.2],seed=0L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[acct_key: int, mrch_brnd_id: int, tran_cnt: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the dataframe to release some memory\n",
    "import gc\n",
    "del dataframe\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Hu's Method **\n",
    "- the simplist model prduces auc of 0.53 with training time 1.5 hours\n",
    "- need to tune parameters to improve performance\n",
    "- need to think about how to address this imbalanced classification problem (majority are 0s, because the objective function is to minimize total squared errors, it tends to predict more 0s to minimize the objective function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "import math\n",
    "\n",
    "als = ALS(rank=10, maxIter=10, regParam=1.0, alpha=1.0, numUserBlocks=10, numItemBlocks=10, \n",
    "          implicitPrefs=True, nonnegative=False, \n",
    "          userCol=\"acct_key\", itemCol=\"mrch_brnd_id\", ratingCol=\"tran_cnt\", \n",
    "          #checkpointInterval=10, \n",
    "          seed=1L, \n",
    "          intermediateStorageLevel=\"DISK_ONLY_2\", finalStorageLevel=\"MEMORY_AND_DISK\")\n",
    "\n",
    "# %time model = als.fit(train) # this doesn't save the model to memory when intermediateStorageLevel=\"DISK_ONLY_2\"\n",
    "# only works when intermediateStorageLevel=\"MEMORY_AND_DISK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4727 seconds\n"
     ]
    }
   ],
   "source": [
    "strt_time = time.time()\n",
    "\n",
    "model = als.fit(train)\n",
    "\n",
    "end_time = time.time()\n",
    "print\"{0:.0f} seconds\" .format(end_time - strt_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.transform(test).cache()\n",
    "# because we have filled all missing entries with 0s, there will be no NaN values in the predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+-------------+\n",
      "|acct_key|mrch_brnd_id|tran_cnt|   prediction|\n",
      "+--------+------------+--------+-------------+\n",
      "|    7417|         137|       0|   0.15832238|\n",
      "|   12006|         137|       0|   0.08355707|\n",
      "|  115363|         137|       0|   0.10356664|\n",
      "|  116319|         137|       0|-0.0010002964|\n",
      "|  203794|         137|       0|  0.040202715|\n",
      "|  219972|         137|       0|   0.06458692|\n",
      "|  225757|         137|       0| 0.0041738767|\n",
      "|  225866|         137|       2|          0.0|\n",
      "|  271143|         137|       0|  0.042408902|\n",
      "|  274111|         137|       0|   0.02704041|\n",
      "+--------+------------+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max score: 2.11\n",
      "min score: -1.14\n",
      "1652 seconds\n"
     ]
    }
   ],
   "source": [
    "strt_time = time.time()\n",
    "\n",
    "# the predicted values are not the probability of a transaction, therefore, the range are not [0.1], but could be > 1 or < 0\n",
    "max_score = pred.select(\"prediction\").rdd.max()[0]\n",
    "min_score = pred.select(\"prediction\").rdd.min()[0]\n",
    "\n",
    "print \"max score: %.2f\" % max_score\n",
    "print \"min score: %.2f\" % min_score\n",
    "\n",
    "end_time = time.time()\n",
    "print\"{0:.0f} seconds\" .format(end_time - strt_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 ms, sys: 2 ms, total: 5 ms\n",
      "Wall time: 14.7 ms\n"
     ]
    }
   ],
   "source": [
    "# because Hu's method output scores are not probability of tran_cnt = 1, we need to map it to range [0,1]\n",
    "# and then apply cutoff threshold to convert it to binary outcome\n",
    "\n",
    "cutoff_threshold = 0.5 # this is the cutoff threshold we want to use for the probability in the range of [0,1]\n",
    "\n",
    "# this is the corresponding cutoff threshold for the scores\n",
    "adj_threshold = cutoff_threshold * (max_score - min_score) + min_score \n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BinaryType, IntegerType, DoubleType\n",
    "\n",
    "udf_score = udf(lambda x: 1.00 if x >= adj_threshold else 0.00, DoubleType()) #Define UDF function\n",
    "\n",
    "%time pred = pred.withColumn('pred_binary',udf_score('prediction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+-------------+-----------+----------+\n",
      "|acct_key|mrch_brnd_id|tran_cnt|   prediction|pred_binary|obs_binary|\n",
      "+--------+------------+--------+-------------+-----------+----------+\n",
      "|    7417|         137|       0|   0.15832238|        0.0|      null|\n",
      "|   12006|         137|       0|   0.08355707|        0.0|      null|\n",
      "|  115363|         137|       0|   0.10356664|        0.0|      null|\n",
      "|  116319|         137|       0|-0.0010002964|        0.0|      null|\n",
      "|  203794|         137|       0|  0.040202715|        0.0|      null|\n",
      "|  219972|         137|       0|   0.06458692|        0.0|      null|\n",
      "|  225757|         137|       0| 0.0041738767|        0.0|      null|\n",
      "|  225866|         137|       2|          0.0|        0.0|      null|\n",
      "|  271143|         137|       0|  0.042408902|        0.0|      null|\n",
      "|  274111|         137|       0|   0.02704041|        0.0|      null|\n",
      "+--------+------------+--------+-------------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 ms, sys: 0 ns, total: 1 ms\n",
      "Wall time: 8.77 ms\n"
     ]
    }
   ],
   "source": [
    "# convert raw tran_cnt into binary data\n",
    "\n",
    "udf_tran_cnt = udf(lambda x: 1.00 if x > 0 else 0.00, DoubleType()) #Define UDF function\n",
    "\n",
    "%time pred = pred.withColumn('obs_binary',udf_tran_cnt('tran_cnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+-------------+-----------+----------+\n",
      "|acct_key|mrch_brnd_id|tran_cnt|   prediction|pred_binary|obs_binary|\n",
      "+--------+------------+--------+-------------+-----------+----------+\n",
      "|    7417|         137|       0|   0.15832238|        0.0|       0.0|\n",
      "|   12006|         137|       0|   0.08355707|        0.0|       0.0|\n",
      "|  115363|         137|       0|   0.10356664|        0.0|       0.0|\n",
      "|  116319|         137|       0|-0.0010002964|        0.0|       0.0|\n",
      "|  203794|         137|       0|  0.040202715|        0.0|       0.0|\n",
      "|  219972|         137|       0|   0.06458692|        0.0|       0.0|\n",
      "|  225757|         137|       0| 0.0041738767|        0.0|       0.0|\n",
      "|  225866|         137|       2|          0.0|        0.0|       1.0|\n",
      "|  271143|         137|       0|  0.042408902|        0.0|       0.0|\n",
      "|  274111|         137|       0|   0.02704041|        0.0|       0.0|\n",
      "+--------+------------+--------+-------------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- acct_key: integer (nullable = true)\n",
      " |-- mrch_brnd_id: integer (nullable = true)\n",
      " |-- tran_cnt: integer (nullable = true)\n",
      " |-- prediction: float (nullable = true)\n",
      " |-- pred_binary: double (nullable = true)\n",
      " |-- obs_binary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are under ROC: 0.53\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"pred_binary\", labelCol=\"obs_binary\",\n",
    "                                          metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(pred)\n",
    "print \"Are under ROC: %.2f\" % auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- this very low ROC may be due to the imbalanced classification problem, where majority of the observations are 0s, very few are 1s, so the optimization tends to predict every outcome to be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total positive observations: 17226369\n"
     ]
    }
   ],
   "source": [
    "%time tot_obs = pred.select(\"obs_binary\").groupBy().sum().collect()[0][0]\n",
    "print \"total positive observations: %d\" % tot_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total observations: 301221717\n",
      "% of positive observations: 5%\n"
     ]
    }
   ],
   "source": [
    "%time tot_pred = pred.count()\n",
    "print \"total observations: %d\" % tot_pred\n",
    "print \"%% of positive observations: %d%%\" % (tot_obs / tot_pred * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total positive predictions: 1865148\n",
      "% of positive observations: 0.62%\n"
     ]
    }
   ],
   "source": [
    "%time pos_pred = pred.select(\"pred_binary\").groupBy().sum().collect()[0][0]\n",
    "print \"total positive predictions: %d\" % pos_pred\n",
    "print \"%% of positive observations: %.2f%%\" % (pos_pred / tot_pred * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Non-Negative Matrix Factorization **\n",
    "- I have tested rank = 10 (1.5 hr) and rank = 30 (3.5 hr), neither predicted any positive numbers (all 0s)\n",
    "- it seems that this method itself is not a good choice, may be helpful when turn on this in Hu's method, which will potentially slows down Hu's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "import math\n",
    "\n",
    "als_nmf = ALS(rank=10, maxIter=10, regParam=1.0, alpha=1.0, numUserBlocks=10, numItemBlocks=10, \n",
    "          implicitPrefs=False, nonnegative=True, \n",
    "          userCol=\"acct_key\", itemCol=\"mrch_brnd_id\", ratingCol=\"tran_cnt\", \n",
    "          checkpointInterval=10, seed=1L, \n",
    "          intermediateStorageLevel=\"DISK_ONLY_2\", finalStorageLevel=\"MEMORY_AND_DISK\")\n",
    "\n",
    "# %time model = als.fit(train) # this doesn't save the model to memory when intermediateStorageLevel=\"DISK_ONLY_2\"\n",
    "# only works when intermediateStorageLevel=\"MEMORY_AND_DISK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4543 seconds\n"
     ]
    }
   ],
   "source": [
    "strt_time = time.time()\n",
    "\n",
    "model_nmf = als_nmf.fit(train)\n",
    "\n",
    "end_time = time.time()\n",
    "print\"{0:.0f} seconds\" .format(end_time - strt_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_nmf = model_nmf.transform(test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 80 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:808)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:806)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1668)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1587)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1833)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1832)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:108)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-e5dac7c59225>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time max_pred_nmf = pred_nmf.select(\"prediction\").rdd.max()[0]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mmax_pred_nmf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/dsap/devl/private/common/python2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2165\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/dsap/devl/private/common/python2.7/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/dsap/devl/private/common/python2.7/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/dsap/devl/private/common/python2.7/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark2-client/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mmax\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1003\u001b[0m         \"\"\"\n\u001b[0;32m   1004\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1005\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark2-client/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    832\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark2-client/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    806\u001b[0m         \"\"\"\n\u001b[0;32m    807\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 808\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    809\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 80 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:808)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:806)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1668)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1587)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1833)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1832)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:108)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "%time max_pred_nmf = pred_nmf.select(\"prediction\").rdd.max()[0]\n",
    "print max_pred_nmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- unfortunately, it did not predict any positive tran_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error: 1.52 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator_nmf = RegressionEvaluator(metricName=\"rmse\", labelCol=\"tran_cnt\",\n",
    "                                    predictionCol=\"prediction\")\n",
    "% time rmse = evaluator_nmf.evaluate(pred_nmf)\n",
    "print \"Root mean squared error: %.2f \" % rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- now let's test rank = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 707 ms, sys: 408 ms, total: 1.11 s\n",
      "Wall time: 3h 31min 2s\n"
     ]
    }
   ],
   "source": [
    "als_nmf2 = ALS(rank=30, maxIter=10, regParam=1.0, alpha=1.0, numUserBlocks=10, numItemBlocks=10, \n",
    "               implicitPrefs=False, nonnegative=True, \n",
    "               userCol=\"acct_key\", itemCol=\"mrch_brnd_id\", ratingCol=\"tran_cnt\", \n",
    "               checkpointInterval=10, seed=1L, \n",
    "               intermediateStorageLevel=\"MEMORY_AND_DISK\", finalStorageLevel=\"MEMORY_AND_DISK\")\n",
    "\n",
    "%time model_nmf2 = als_nmf2.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time pred_nmf2 = model_nmf2.transform(test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69 ms, sys: 27 ms, total: 96 ms\n",
      "Wall time: 11min 12s\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "%time max_pred_nmf2 = pred_nmf2.select(\"prediction\").rdd.max()[0]\n",
    "print max_pred_nmf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- again, no positive predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Hu's method parameter tuning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert raw tran_cnt into binary data\n",
    "%time train = train.withColumn('tran_cnt_binary',udf_tran_cnt('tran_cnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "als_hu = ALS()\n",
    "\n",
    "# parameter setting for initial tuning\n",
    "paramgrid = ParamGridBuilder() \\\n",
    "    .baseOn([als.numUserBlocks,-1]) \\\n",
    "    .baseOn([als.numItemBlocks,-1]) \\\n",
    "    .baseOn([als.userCol,\"acct_key\"]) \\\n",
    "    .baseOn([als.itemCol,\"mrch_brnd_id\"]) \\\n",
    "    .baseOn([als.ratingCol,\"tran_cnt\"])\\\n",
    "    .baseOn([als.implicitPrefs, True]) \\\n",
    "    .baseOn([als.nonnegative,True]) \\\n",
    "    .baseOn([als.seed,1L]) \\\n",
    "    .baseOn([als.intermediateStorageLevel, \"MEMORY_AND_DISK\"]) \\\n",
    "    .baseOn([als.finalStorageLevel,\"MEMORY_AND_DISK\"]) \\\n",
    "    .addGrid(als.rank, [30,50]) \\\n",
    "    .addGrid(als.regParam, [1.0,10.0]) \\\n",
    "    .addGrid(als.maxIter, 30) \\\n",
    "    .addGrid(als.alpha, [0.1, 0.5]) \\\n",
    "    .build()\n",
    "\n",
    "eval_hu = BinaryClassificationEvaluator(rawPredictionCol=\"pred_binary\", labelCol=\"tran_cnt_binary\",\n",
    "                                        metricName=\"areaUnderROC\")\n",
    "# how to create \"pred_binary\" column here? We did not specify the predicton dataframe.\n",
    "\n",
    "# using RegressionEvaluator RMSE between tran_cnt and score seems not a reasonable choice\n",
    "\n",
    "grid_hu = TrainValidationSplit(estimator=als_hu, estimatorParamMaps=paramgrid, evaluator=eval_hu, trainRatio=0.8)\n",
    "model_hu = grid_hu.fit(train) # we should use training data here instead of dataframe, otherwise we are vulnerable to overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the best model from model_hu\n",
    "# test performance on test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
